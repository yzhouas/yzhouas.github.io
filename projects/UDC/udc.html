<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
      <title>Under-Display Camera (UDC)</title>
      <meta name="description" content="">
      <meta name="keywords" content="">
      <!-- Fonts and stuff -->
      <link href="./files/project.css" rel="stylesheet">
      <link href="./files/iconize.css" rel="stylesheet">
      <!--<link href="./images/new_icon.png" rel="shortcut icon">-->
      <script type="text/javascript" async="" src="./files/ga.js"></script>
      <link href="http://twitter.github.com/bootstrap/assets/ico/apple-touch-icon-144-precomposed.png" rel="apple-touch-icon-precomposed" sizes="144x144">
      <link href="http://twitter.github.com/bootstrap/assets/ico/apple-touch-icon-114-precomposed.png" rel="apple-touch-icon-precomposed" sizes="114x114">
      <link href="http://twitter.github.com/bootstrap/assets/ico/apple-touch-icon-72-precomposed.png" rel="apple-touch-icon-precomposed" sizes="72x72">
      <link href="http://twitter.github.com/bootstrap/assets/ico/apple-touch-icon-57-precomposed.png" rel="apple-touch-icon-precomposed">
   </head>
   <body>
      <div id="content">
         <div id="content-inner">
            <div class="section head">
               <p>
               <h1 id="Under-display Camera">Image Restoration for Under-Display Camera</h1>
               </p>
               <div class="affiliations">
                  <p><a href="https://yzhouas.github.io/">Yuqian Zhou</a> &nbsp; &nbsp; &nbsp; &nbsp;
                     <a href="https://www.linkedin.com/in/davidren1993/">David Ren</a> &nbsp; &nbsp; &nbsp; &nbsp;
                     <a href="https://www.microsoft.com/en-us/research/people/neilem/">Neil Emerton</a>&nbsp; &nbsp; &nbsp; &nbsp;
                     <a href="https://www.microsoft.com/en-us/research/people/sehlim/">Sehoon Lim</a>&nbsp; &nbsp; &nbsp; &nbsp;
                     <a href="https://www.microsoft.com/en-us/research/people/tlarge/">Timothy Large</a>
                  <p><a href="https://ifp-uiuc.github.io/">IFP,UIUC  </a >&nbsp; &nbsp; &nbsp; &nbsp;
                     <a href="http://www.laurawaller.com/">CIL,UC Berkeley  </a >&nbsp; &nbsp; &nbsp; &nbsp;
                     <a href="https://www.microsoft.com/en-us/research/lab/applied-sciences-lab/">Applied Science Group, Microsoft</a >
               </div>
               <ul id="tabs">
                  <li><a href="udc.html" name="#tab1" id="current">Home</a></li>
                  <li><a href="" name="#tab3">Challenge</a></li>
               </ul>
            </div>
            <center><img style="width: 70%;" src="./images/udc/udc.png" alt="QMUL SurvFace"></center>
            <h2 id="Abstract">Abstract</h2>
            <p>The new trend of full-screen devices encourages us to position a camera behind a screen. Removing the bezel and centralizing the camera under the screen brings larger display-to-body ratio and enhances eye contact in video chat, but also causes image degradation. In this paper, we focus on a newly-defined Under-Display Camera (UDC), as a novel real-world single image restoration problem. First, we take a 4k Transparent OLED (T-OLED) and a phone Pentile OLED (P-OLED) and analyze their optical systems to understand the degradation. Second, we design a novel Monitor-Camera Imaging System (MCIS) for easier real pair data acquisition, and a model-based data synthesizing pipeline to generate UDC data only from display pattern and camera measurements. Finally, we resolve the complicated degradation using learning-based methods. Our model demonstrates a real-time high-quality restoration trained with either real or the synthetic data. The presented results and methods provide good practice to apply image restoration to real-world applications.
            </p>
            <center><img style="width: 65%;" src="./images/udc/data.png" alt="QMUL SurvFace"></center>
            <h2 id="UDC Dataset">UDC Dataset</h2>
            <p>The new dataset is collected by a Monitor-Camera Imaging System (MCIS). We utilized 300 images form DIV2K dataset. For each display type: T-OLED, and P-OLED, we collected paired display-free and display-covered imaging data in the form of both 16-bit RAW sensor data, and 8-bit RGB. Images have resolution of 1024*2048. We only release the paired RGB data for training. Our RGB is linear, so users can reverse the process to generate 8-bit RAW sensor data. Please refer to our <strong><a href="https://arxiv.org/abs/2003.04857">report</a></strong> for more details of the dataset.
            </p>
            
            <h2 id="news">News</h2>
            <p>
            <ul>
			<li><strong>Feb. 28, 2020:</strong> The UDC paper is accepted by CVPR 2021. The validation and testing dataset are released for evaluation.</li>
			<li><strong>Aug. 30, 2020:</strong> The <a href="https://link.springer.com/chapter/10.1007/978-3-030-68238-5_26">UDC ECCV Challenge Report</a> is published in ECCV Workshop Proceedings. Congratulations to all the winners.</li>	
	       <li><strong>March 21, 2020:</strong> We organized the <a href="https://rlq-tod.github.io/index.html" target="_blank">Image Restoration for UDC Challenge in conjunction with the ECCV'20 Workshop and Challenge on Real-World Recognition from Low-Quality Inputs (RLQ)</a>.  Winners were awarded with prize. The challenge is sponsored by Microsoft.</li>
            </ul>
            </p>
            <h2 id="download">Download</h2>
	   
	    <p> We release the training and testing dataset the same as the held ECCV UDC challenge. The training data consists of 240 pairs of 1024*2048 images, totally 480 images. Validation inputs consist of 30 images of resolution 1024*2048, and construct in the form '.mat'. The key of the variable is 'val_display' and the variable is a 'uint8' array of size [30, 1024, 2048, 3]. You can first extract each image and test it using your pre-trained model. The testing dataset has the same structure as the validation data, but with another 30 images. For a fair comparison with other methods, the model should not be trained on the validation data, and the results should be reported on the testing data. We also prepare the evaluation code used in the challenge.
	   
	        <ul>
	          <li>UDC Training Dataset (RGB, 2.1G): 
               [<a href="https://1drv.ms/u/s!At4CtN8cceVdbASQv94dYsMOkSk" target="_blank">Microsoft OneDrive</a>]
               [<a href="https://drive.google.com/file/d/1zB1xoxKBghTTq0CKU1VghBoAoQc5YlHk/view?usp=sharing" target="_blank">Google Drive</a>]
               [<a href="https://pan.baidu.com/s/1Px6fWzVtBZygPh03C3KftQ" target="_blank">Baidu Drive (pw:9k7q)</a>]</li>
				
			   <li>
				UDC Validation and Testing Dataset and evaluation codes (RGB, 1.2G):
			   [<a href="https://1drv.ms/u/s!At4CtN8cceVdbTR1bFO708hqPLw?e=qW8uMZ" target="_blank">Microsoft OneDrive</a>]
               [<a href="https://drive.google.com/file/d/171_-iTN7bIhLHMZiGHV8zgXtoTG3T297/view?usp=sharing" target="_blank">Google Drive</a>]
               [<a href="https://pan.baidu.com/s/1dm2zNhbPObZ66Rg4uvn7lg" target="_blank">Baidu Drive (pw:zvyc)</a>]</li>
				   
				   
				
			   </li>
				
	       </ul>
	    </p>
            <h2 id="citation">Citation</h2>
            <pre>
	    	Image Restoration for Under-Display Camera.
		Zhou, Yuqian and Ren, David and Emerton, Neil and Lim, Sehoon and Large, Timothy.
		The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021. <a href="https://arxiv.org/abs/2003.04857">arXiv</a> <a href="UDC_bib.bib">Bibtex</a>
	    </pre>

        
            <h2 id="Licence">Licence</h2>
            <p>UDC dataset can only be used for research purposes. All the images are collected from DIV2K dataset.
               The copyright belongs to Microsoft and the original dataset owners.
            </p>
            <h2 id="contact">Contact</h2>
            <p>Should you have any questions, please contact Yuqian Zhou via <strong>zhouyuqian133@gmail.com</strong>.</p>
            <div class="footer">
	        <div class="clearfix">
		  <div class="leftbox">
<script type="text/javascript" id="clstr_globe" src="//cdn.clustrmaps.com/globe.js?d=EZLl6wJ5ko84L-fE6smgp5TjokpvwJ3pebeEstJRTWA"></script>
		  </div>
		</div>
           </div>
         </div>
         
      </div>
   </body>
</html>
